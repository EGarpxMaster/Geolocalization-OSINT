"""
PIPELINE DE ENTRENAMIENTO - GEOLOCALIZATION OSINT
===================================================
Sistema unificado para anotaci√≥n manual y fine-tuning del modelo CLIP.

Uso:
    streamlit run training_pipeline.py
"""

import os
import json
import csv
from pathlib import Path
from datetime import datetime
import sys

# Verificar dependencias
try:
    import streamlit as st
    from PIL import Image
    import pandas as pd
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    from transformers import CLIPProcessor, CLIPModel
    from tqdm import tqdm
except ImportError as e:
    print(f"‚ùå Error: Falta instalar dependencias: {e}")
    print("üí° Ejecuta: pip install streamlit pillow pandas torch transformers tqdm")
    sys.exit(1)

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
MINING_DIR = DATA_DIR / "mining"
IMAGES_DIR = MINING_DIR / "images"
METADATA_FILE = MINING_DIR / "metadata.json"
METADATA_CSV = MINING_DIR / "metadata.csv"
ANNOTATIONS_FILE = MINING_DIR / "annotations.json"
ANNOTATIONS_CSV = MINING_DIR / "annotations.csv"
CITIES_CSV = DATA_DIR / "cities_mx.csv"
MODEL_DIR = BASE_DIR / "model"
CHECKPOINT_DIR = MODEL_DIR / "checkpoints"

# Crear directorios
MODEL_DIR.mkdir(exist_ok=True)
CHECKPOINT_DIR.mkdir(exist_ok=True)

# ============================================================================
# FUNCIONES DE UTILIDAD
# ============================================================================

def load_metadata_from_csv():
    """Carga metadata desde CSV"""
    images = []
    try:
        with open(METADATA_CSV, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                images.append({
                    'filename': row['filename'],
                    'source': row['source'],
                    'photo_id': row.get('photo_id', ''),
                    'city': row['city'],
                    'state': row['state'],
                    'city_target': row['city'],
                    'state_target': row['state'],
                    'lat': float(row['lat']) if row['lat'] else 0.0,
                    'lon': float(row['lon']) if row['lon'] else 0.0,
                    'url': row.get('url', ''),
                    'title': row.get('title', ''),
                    'photographer': row.get('photographer', ''),
                    'local_path': str(IMAGES_DIR / row['filename'])
                })
    except Exception as e:
        st.error(f"Error cargando CSV: {e}")
    return images

def load_metadata_from_json():
    """Carga metadata desde JSON (fallback)"""
    images = []
    try:
        with open(METADATA_FILE, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Estructura simple: metadata['images']
        if 'images' in metadata and isinstance(metadata['images'], list):
            images = metadata['images']
        
        # Estructura compleja: cities -> state -> city -> images
        elif 'cities' in metadata:
            for state_data in metadata['cities'].values():
                if isinstance(state_data, dict):
                    for city_data in state_data.values():
                        if isinstance(city_data, dict) and 'images' in city_data:
                            images.extend(city_data['images'])
        
        # Normalizar filenames
        for img in images:
            if isinstance(img, dict):
                if 'filename' not in img and 'local_path' in img:
                    img['filename'] = Path(img['local_path']).name
    except Exception as e:
        st.error(f"Error cargando JSON: {e}")
    return images

# ============================================================================
# INTERFAZ PRINCIPAL STREAMLIT
# ============================================================================

def main_interface():
    """Interfaz principal unificada de Streamlit"""
    
    st.set_page_config(
        page_title="Pipeline de Entrenamiento OSINT",
        page_icon="üéì",
        layout="wide"
    )
    
    st.title("üéì Pipeline de Entrenamiento - Geolocalizador OSINT")
    
    # Sidebar con selecci√≥n de modo
    with st.sidebar:
        st.header("‚öôÔ∏è Modo de Operaci√≥n")
        mode = st.radio(
            "Selecciona el modo:",
            ["üìù Anotaci√≥n", "üî¨ Fine-tuning", "üèóÔ∏è Regenerar Modelo", "üìä Estad√≠sticas", "üéØ Evaluaci√≥n"],
            index=0
        )
        
        st.divider()
        
        # Mostrar stats generales
        if METADATA_CSV.exists():
            with open(METADATA_CSV, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                total_images = sum(1 for _ in reader)
            st.metric("Im√°genes minadas", total_images)
        elif METADATA_FILE.exists():
            with open(METADATA_FILE, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            st.metric("Im√°genes minadas", len(metadata.get('images', [])))
        
        if ANNOTATIONS_FILE.exists():
            with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
                annotations = json.load(f)
            st.metric("Im√°genes anotadas", len(annotations.get('images', [])))
    
    # Mostrar interfaz seg√∫n modo
    if mode == "üìù Anotaci√≥n":
        show_annotation_interface()
    elif mode == "üî¨ Fine-tuning":
        show_training_interface()
    elif mode == "üèóÔ∏è Regenerar Modelo":
        show_build_model_interface()
    elif mode == "üìä Estad√≠sticas":
        show_statistics_interface()
    else:
        show_evaluation_interface()

def show_annotation_interface():
    """Interfaz de anotaci√≥n mejorada con etiquetas personalizadas"""
    
    st.header("üìù Anotaci√≥n de Im√°genes")
    st.markdown("Mejora el modelo agregando etiquetas descriptivas y verificando la calidad de las im√°genes.")
    
    # Cargar datos desde CSV (preferido) o JSON (fallback)
    if METADATA_CSV.exists():
        images = load_metadata_from_csv()
    elif METADATA_FILE.exists():
        images = load_metadata_from_json()
    else:
        st.error("‚ùå No hay im√°genes descargadas. Ejecuta primero `mining_pipeline.py`")
        st.code("python mining_pipeline.py --mode all --images 20", language="bash")
        return
    
    if not images:
        st.warning("‚ö†Ô∏è No hay im√°genes para anotar")
        return
    
    # Cargar anotaciones existentes
    if ANNOTATIONS_FILE.exists():
        with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
    else:
        annotations = {'images': [], 'deleted_images': []}
    
    # Normalizar filenames en anotaciones
    for ann in annotations['images']:
        if 'filename' not in ann and 'local_path' in ann:
            ann['filename'] = Path(ann['local_path']).name
    
    annotated_files = {ann.get('filename', '') for ann in annotations['images']}
    deleted_files = set(annotations.get('deleted_images', []))
    pending_images = [img for img in images if img.get('filename', '') not in annotated_files and img.get('filename', '') not in deleted_files]
    
    # Estad√≠sticas en cards
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("üì• Total", len(images))
    col2.metric("‚úÖ Anotadas", len(annotated_files))
    col3.metric("üóëÔ∏è Eliminadas", len(deleted_files))
    col4.metric("‚è≥ Pendientes", len(pending_images))
    
    if not pending_images:
        st.success("‚úÖ ¬°Todas las im√°genes est√°n procesadas!")
        st.balloons()
        
        if st.button("üîÑ Revisar im√°genes anotadas"):
            st.session_state.review_mode = True
            st.rerun()
        return
    
    # Inicializar estado
    if 'current_idx' not in st.session_state:
        st.session_state.current_idx = 0
    
    if st.session_state.current_idx >= len(pending_images):
        st.session_state.current_idx = len(pending_images) - 1
    
    current_img = pending_images[st.session_state.current_idx]
    
    # Normalizar filename si viene de local_path
    if 'filename' not in current_img and 'local_path' in current_img:
        current_img['filename'] = Path(current_img['local_path']).name
    
    # Verificar que la imagen tenga el campo filename
    if 'filename' not in current_img:
        st.error(f"‚ùå Error: Imagen sin nombre de archivo en metadata")
        st.json(current_img)
        return
    
    # Determinar ruta de la imagen
    img_path = IMAGES_DIR / current_img['filename']
    
    # Si no existe, intentar con local_path
    if not img_path.exists() and 'local_path' in current_img:
        img_path = Path(current_img['local_path'])
        if not img_path.is_absolute():
            img_path = Path.cwd() / img_path
    
    st.divider()
    
    # Layout principal
    col_left, col_right = st.columns([3, 2])
    
    with col_left:
        st.subheader(f"üñºÔ∏è Imagen {st.session_state.current_idx + 1} de {len(pending_images)}")
        
        if img_path.exists():
            image = Image.open(img_path)
            st.image(image, use_container_width=True)
            
            # Metadata de la imagen
            with st.expander("üìã Metadata de la imagen", expanded=True):
                # Mostrar ciudad prominentemente
                city_name = current_img.get('city_target', current_img.get('city', 'Desconocida'))
                state_name = current_img.get('state_target', current_img.get('state', ''))
                st.markdown(f"### üìç {city_name}, {state_name}")
                
                col_m1, col_m2 = st.columns(2)
                with col_m1:
                    st.write(f"**Archivo:** `{current_img.get('filename', 'Sin nombre')}`")
                    st.write(f"**Fuente:** {current_img.get('source', 'Desconocida')}")
                    st.write(f"**Tama√±o:** {current_img.get('width', 0)} √ó {current_img.get('height', 0)} px")
                with col_m2:
                    st.write(f"**Coordenadas:** ({current_img.get('lat', 0):.4f}, {current_img.get('lon', 0):.4f})")
                    size_kb = current_img.get('size', 0) / 1024 if current_img.get('size') else 0
                    st.write(f"**Tama√±o archivo:** {size_kb:.1f} KB")
                    st.write(f"**URL original:** [{current_img.get('title', 'Ver')}]({current_img.get('url', '#')})")
        else:
            st.error(f"‚ùå Imagen no encontrada: {current_img.get('filename', 'sin nombre')}")
            if st.button("Marcar como perdida y continuar"):
                annotations.setdefault('deleted_images', []).append(current_img.get('filename', ''))
                save_annotations(annotations)
                st.session_state.current_idx += 1
                st.rerun()
            return
    
    with col_right:
        st.subheader("‚úèÔ∏è Formulario de Anotaci√≥n")
        
        # Ciudad esperada (usar city_target si existe, sino city)
        city_name = current_img.get('city_target', current_img.get('city', 'Desconocida'))
        state_name = current_img.get('state_target', current_img.get('state', ''))
        st.info(f"**Ciudad esperada:** {city_name}, {state_name}")
        
        # Verificaci√≥n
        correct_city = st.radio(
            "¬øLa imagen corresponde a esta ciudad?",
            options=["‚úÖ S√≠", "‚ùå No", "ü§î No estoy seguro"],
            horizontal=True,
            key="city_verification"
        )
        
        # Calidad
        quality = st.select_slider(
            "Calidad de la imagen",
            options=[1, 2, 3, 4, 5],
            value=3,
            format_func=lambda x: ["‚≠ê Muy baja", "‚≠ê‚≠ê Baja", "‚≠ê‚≠ê‚≠ê Media", "‚≠ê‚≠ê‚≠ê‚≠ê Buena", "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente"][x-1]
        )
        
        # Etiquetas personalizadas
        st.markdown("**üè∑Ô∏è Etiquetas personalizadas:**")
        st.caption("Agrega palabras clave que describan la imagen (separadas por comas)")
        custom_tags = st.text_input(
            "Etiquetas",
            placeholder="arquitectura colonial, plaza central, catedral, z√≥calo...",
            help="Estas etiquetas mejorar√°n la precisi√≥n del modelo"
        )
        
        # Elementos detectados
        st.markdown("**üëÅÔ∏è Elementos visibles:**")
        col_e1, col_e2, col_e3 = st.columns(3)
        
        with col_e1:
            has_landmarks = st.checkbox("üèõÔ∏è Monumentos")
            has_architecture = st.checkbox("üèòÔ∏è Arquitectura")
            has_signs = st.checkbox("ü™ß Se√±ales")
        
        with col_e2:
            has_nature = st.checkbox("üå≥ Naturaleza")
            has_urban = st.checkbox("üèôÔ∏è Urbano")
            has_beach = st.checkbox("üèñÔ∏è Playa")
        
        with col_e3:
            has_people = st.checkbox("üë• Personas")
            has_vehicles = st.checkbox("üöó Veh√≠culos")
            has_text = st.checkbox("üìù Texto")
        
        # Confianza
        confidence = st.slider(
            "üéØ Tu confianza en esta anotaci√≥n",
            min_value=0,
            max_value=100,
            value=80,
            help="0% = No estoy seguro, 100% = Muy seguro"
        )
        
        # Notas
        notes = st.text_area(
            "üìù Notas adicionales (opcional)",
            placeholder="Describe lo que ves: monumentos espec√≠ficos, caracter√≠sticas √∫nicas, clima, √©poca del a√±o...",
            height=100
        )
        
        # Anotador fijo
        annotator = "Emma"
        
        st.divider()
        
        # Botones de acci√≥n
        col_b1, col_b2, col_b3, col_b4 = st.columns(4)
        
        with col_b1:
            if st.button("‚¨ÖÔ∏è Anterior", use_container_width=True, disabled=(st.session_state.current_idx == 0)):
                st.session_state.current_idx -= 1
                st.rerun()
        
        with col_b2:
            if st.button("üíæ Guardar", type="primary", use_container_width=True):
                # Guardar anotaci√≥n (usar city_target/state_target si existen)
                annotation = {
                    'filename': current_img.get('filename', ''),
                    'city': current_img.get('city_target', current_img.get('city', '')),
                    'state': current_img.get('state_target', current_img.get('state', '')),
                    'lat': current_img.get('lat', 0),
                    'lon': current_img.get('lon', 0),
                    'source': current_img.get('source', ''),
                    'correct_city': correct_city.split()[0],  # ‚úÖ, ‚ùå, o ü§î
                    'quality': quality,
                    'custom_tags': [tag.strip() for tag in custom_tags.split(',') if tag.strip()],
                    'elements': {
                        'landmarks': has_landmarks,
                        'architecture': has_architecture,
                        'signs': has_signs,
                        'nature': has_nature,
                        'urban': has_urban,
                        'beach': has_beach,
                        'people': has_people,
                        'vehicles': has_vehicles,
                        'text': has_text
                    },
                    'confidence': confidence,
                    'notes': notes,
                    'annotated_at': datetime.now().isoformat(),
                    'annotated_by': annotator
                }
                
                annotations['images'].append(annotation)
                save_annotations(annotations)
                
                # Guardar tambi√©n en CSV
                save_annotation_to_csv(annotation)
                
                st.success("‚úÖ Anotaci√≥n guardada (JSON + CSV)")
                
                # Siguiente imagen
                if st.session_state.current_idx < len(pending_images) - 1:
                    st.session_state.current_idx += 1
                    st.rerun()
                else:
                    st.balloons()
                    st.success("üéâ ¬°Todas las im√°genes anotadas!")
        
        with col_b3:
            if st.button("‚è≠Ô∏è Omitir", use_container_width=True):
                if st.session_state.current_idx < len(pending_images) - 1:
                    st.session_state.current_idx += 1
                    st.rerun()
        
        with col_b4:
            if st.button("üóëÔ∏è Eliminar", use_container_width=True, help="Eliminar imagen de baja calidad"):
                if st.checkbox("Confirmar eliminaci√≥n", key="confirm_delete"):
                    # Agregar a lista de eliminadas
                    annotations.setdefault('deleted_images', []).append(current_img.get('filename', ''))
                    save_annotations(annotations)
                    
                    # Eliminar archivo f√≠sico
                    try:
                        img_path.unlink()
                        st.success(f"üóëÔ∏è Imagen eliminada: {current_img.get('filename', 'imagen')}")
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è No se pudo eliminar el archivo f√≠sico: {e}")
                    
                    st.session_state.current_idx = min(st.session_state.current_idx, len(pending_images) - 2)
                    st.rerun()
    
    # Barra de progreso
    progress = (st.session_state.current_idx + 1) / len(pending_images)
    st.progress(progress)
    st.caption(f"Progreso: {progress*100:.1f}% ({st.session_state.current_idx + 1}/{len(pending_images)})")

def save_annotations(annotations):
    """Guarda anotaciones con backup"""
    try:
        # Crear backup
        if ANNOTATIONS_FILE.exists():
            backup_file = ANNOTATIONS_FILE.with_suffix('.backup.json')
            ANNOTATIONS_FILE.rename(backup_file)
        
        # Guardar nuevo
        with open(ANNOTATIONS_FILE, 'w', encoding='utf-8') as f:
            json.dump(annotations, f, indent=2, ensure_ascii=False)
    except Exception as e:
        st.error(f"Error guardando anotaciones: {e}")

def save_annotation_to_csv(annotation):
    """Guarda una anotaci√≥n en formato CSV para fine-tuning"""
    import csv
    
    # Crear CSV si no existe
    file_exists = ANNOTATIONS_CSV.exists()
    
    with open(ANNOTATIONS_CSV, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Escribir encabezado si es nuevo archivo
        if not file_exists:
            writer.writerow([
                'filename', 'city', 'state', 'lat', 'lon',
                'correct_city', 'quality', 'confidence',
                'landmarks', 'architecture', 'signs', 'nature', 'urban', 'beach',
                'people', 'vehicles', 'text', 'custom_tags', 'notes',
                'annotated_at', 'annotated_by'
            ])
        
        # Escribir datos
        elements = annotation.get('elements', {})
        tags_str = ','.join(annotation.get('custom_tags', []))
        
        writer.writerow([
            annotation.get('filename', ''),
            annotation.get('city', ''),
            annotation.get('state', ''),
            annotation.get('lat', 0.0),
            annotation.get('lon', 0.0),
            annotation.get('correct_city', ''),
            annotation.get('quality', 0),
            annotation.get('confidence', 0),
            elements.get('landmarks', False),
            elements.get('architecture', False),
            elements.get('signs', False),
            elements.get('nature', False),
            elements.get('urban', False),
            elements.get('beach', False),
            elements.get('people', False),
            elements.get('vehicles', False),
            elements.get('text', False),
            tags_str,
            annotation.get('notes', ''),
            annotation.get('annotated_at', ''),
            annotation.get('annotated_by', 'Emma')
        ])


def show_training_interface():
    """Interfaz para ejecutar el fine-tuning"""
    
    st.header("üî¨ Fine-tuning del Modelo")
    
    # Verificar anotaciones
    if not ANNOTATIONS_FILE.exists():
        st.warning("‚ö†Ô∏è No hay anotaciones disponibles")
        st.info("Primero debes anotar im√°genes en la secci√≥n **üìù Anotaci√≥n**")
        return
    
    with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    
    total_annotations = len(annotations.get('images', []))
    
    if total_annotations < 50:
        st.warning(f"‚ö†Ô∏è Solo tienes {total_annotations} anotaciones")
        st.info("Se recomiendan al menos 100 anotaciones para mejores resultados, pero puedes intentar con 50+")
    
    st.success(f"‚úÖ {total_annotations} im√°genes anotadas disponibles")
    
    # Configuraci√≥n de entrenamiento
    st.subheader("‚öôÔ∏è Configuraci√≥n")
    
    col1, col2 = st.columns(2)
    
    with col1:
        epochs = st.number_input("√âpocas", min_value=1, max_value=20, value=5)
        batch_size = st.select_slider("Batch Size", options=[2, 4, 8, 16], value=8)
        learning_rate = st.select_slider("Learning Rate", options=[1e-6, 5e-6, 1e-5, 5e-5], value=1e-5, format_func=lambda x: f"{x:.0e}")
    
    with col2:
        min_quality = st.slider("Calidad m√≠nima", 1, 5, 2)
        min_confidence = st.slider("Confianza m√≠nima (%)", 0, 100, 50)
    
    st.divider()
    
    # Estimaci√≥n
    device = "GPU" if torch.cuda.is_available() else "CPU"
    st.info(f"üñ•Ô∏è **Dispositivo:** {device}")
    
    if device == "CPU":
        st.warning("‚ö†Ô∏è Entrenamiento en CPU ser√° lento. Considera usar Google Colab con GPU.")
    
    estimated_time_min = epochs * (5 if device == "GPU" else 15)
    st.info(f"‚è±Ô∏è **Tiempo estimado:** {estimated_time_min}-{estimated_time_min*2} minutos")
    
    # Bot√≥n de entrenamiento
    if st.button("üöÄ Iniciar Fine-tuning", type="primary", use_container_width=True):
        with st.spinner("Entrenando modelo... Esto puede tardar varios minutos."):
            try:
                result = finetune_model(
                    epochs=epochs,
                    batch_size=batch_size,
                    learning_rate=learning_rate,
                    min_quality=min_quality,
                    min_confidence=min_confidence
                )
                
                if result:
                    st.success("‚úÖ Fine-tuning completado exitosamente!")
                    st.balloons()
                    st.info("üí° Siguiente paso: Ve a **üèóÔ∏è Regenerar Modelo** para actualizar los embeddings")
                else:
                    st.error("‚ùå Error durante el entrenamiento")
            except Exception as e:
                st.error(f"‚ùå Error: {e}")
                st.exception(e)

def show_build_model_interface():
    """Interfaz para regenerar embeddings"""
    
    st.header("üèóÔ∏è Regenerar Embeddings del Modelo")
    
    finetuned_path = MODEL_DIR / "modelo_finetuned.pth"
    
    if not finetuned_path.exists():
        st.warning("‚ö†Ô∏è No existe modelo fine-tuned")
        st.info("Primero debes ejecutar el **üî¨ Fine-tuning** del modelo")
        return
    
    st.success("‚úÖ Modelo fine-tuned encontrado")
    
    st.markdown("""
    Este proceso:
    1. Carga el modelo CLIP fine-tuned
    2. Genera embeddings mejorados para todas las ciudades
    3. Guarda el modelo actualizado en `model/modelo.pth`
    
    **Tiempo estimado:** 3-5 minutos
    """)
    
    if st.button("üèóÔ∏è Regenerar Embeddings", type="primary", use_container_width=True):
        with st.spinner("Regenerando embeddings de ciudades..."):
            try:
                build_model()
                st.success("‚úÖ Embeddings regenerados exitosamente!")
                st.balloons()
                st.info("üí° Ahora puedes probar el modelo mejorado ejecutando: `streamlit run Geolocalizador.py`")
            except Exception as e:
                st.error(f"‚ùå Error: {e}")
                st.exception(e)

def show_statistics_interface():
    """Interfaz de estad√≠sticas"""
    
    st.header("üìä Estad√≠sticas del Dataset")
    
    # Cargar datos
    stats = {}
    
    if METADATA_CSV.exists():
        with open(METADATA_CSV, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            images = list(reader)
            stats['total_images'] = len(images)
            stats['total_cities'] = len(set(img['city'] for img in images if img.get('city')))
    elif METADATA_FILE.exists():
        with open(METADATA_FILE, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        stats['total_images'] = len(metadata.get('images', []))
        stats['total_cities'] = len(metadata.get('cities', {}))
    
    if ANNOTATIONS_FILE.exists():
        with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
        stats['annotated'] = len(annotations.get('images', []))
        stats['deleted'] = len(annotations.get('deleted_images', []))
    else:
        stats['annotated'] = 0
        stats['deleted'] = 0
    
    # M√©tricas principales
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("üñºÔ∏è Im√°genes totales", stats.get('total_images', 0))
    col2.metric("‚úÖ Anotadas", stats.get('annotated', 0))
    col3.metric("üóëÔ∏è Eliminadas", stats.get('deleted', 0))
    col4.metric("üèôÔ∏è Ciudades", stats.get('total_cities', 0))
    
    if not ANNOTATIONS_FILE.exists():
        st.info("No hay estad√≠sticas de anotaciones disponibles")
        return
    
    # Gr√°ficos
    st.divider()
    
    with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    
    if not annotations.get('images'):
        st.info("A√∫n no hay im√°genes anotadas")
        return
    
    # Distribuci√≥n de calidad
    st.subheader("üìà Distribuci√≥n de Calidad")
    quality_dist = {}
    for ann in annotations['images']:
        q = ann.get('quality', 0)
        quality_dist[q] = quality_dist.get(q, 0) + 1
    
    quality_df = pd.DataFrame([
        {'Calidad': f"{'‚≠ê' * k} ({k})", 'Cantidad': v}
        for k, v in sorted(quality_dist.items())
    ])
    st.bar_chart(quality_df.set_index('Calidad'))
    
    # Distribuci√≥n por ciudad
    st.subheader("üó∫Ô∏è Top 10 Ciudades Anotadas")
    city_dist = {}
    for ann in annotations['images']:
        city_key = f"{ann.get('city')}, {ann.get('state')}"
        city_dist[city_key] = city_dist.get(city_key, 0) + 1
    
    top_cities = sorted(city_dist.items(), key=lambda x: x[1], reverse=True)[:10]
    city_df = pd.DataFrame(top_cities, columns=['Ciudad', 'Anotaciones'])
    st.dataframe(city_df, use_container_width=True, hide_index=True)
    
    # Elementos detectados
    st.subheader("üëÅÔ∏è Elementos M√°s Comunes")
    elements_count = {}
    for ann in annotations['images']:
        for element, value in ann.get('elements', {}).items():
            if value:
                elements_count[element] = elements_count.get(element, 0) + 1
    
    if elements_count:
        elements_df = pd.DataFrame([
            {'Elemento': k, 'Frecuencia': v}
            for k, v in sorted(elements_count.items(), key=lambda x: x[1], reverse=True)
        ])
        st.bar_chart(elements_df.set_index('Elemento'))
    
    # Etiquetas personalizadas m√°s usadas
    st.subheader("üè∑Ô∏è Etiquetas Personalizadas Populares")
    tags_count = {}
    for ann in annotations['images']:
        for tag in ann.get('custom_tags', []):
            if tag:
                tags_count[tag] = tags_count.get(tag, 0) + 1
    
    if tags_count:
        top_tags = sorted(tags_count.items(), key=lambda x: x[1], reverse=True)[:15]
        tags_df = pd.DataFrame(top_tags, columns=['Etiqueta', 'Uso'])
        st.dataframe(tags_df, use_container_width=True, hide_index=True)
    else:
        st.info("No se han agregado etiquetas personalizadas a√∫n")

# ============================================================================
# DATASET PERSONALIZADO
# ============================================================================

class GeoDataset(Dataset):
    """Dataset personalizado para fine-tuning de CLIP"""
    
    def __init__(self, annotations, processor, min_quality=2, min_confidence=50):
        self.processor = processor
        self.samples = []
        
        # Cargar ciudades
        cities = {}
        with open(CITIES_CSV, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                cities[f"{row['name']}_{row['state']}"] = row
        
        # Filtrar por calidad y confianza
        for ann in annotations['images']:
            if (ann.get('quality', 0) >= min_quality and 
                ann.get('confidence', 0) >= min_confidence and
                ann.get('correct_city') == 'S√≠'):
                
                img_path = IMAGES_DIR / ann['filename']
                if img_path.exists():
                    city_key = f"{ann['city']}_{ann['state']}"
                    if city_key in cities:
                        self.samples.append({
                            'image_path': img_path,
                            'city': ann['city'],
                            'state': ann['state'],
                            'tags': cities[city_key].get('tags', '')
                        })
        
        print(f"‚úÖ Dataset creado: {len(self.samples)} im√°genes v√°lidas")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # Cargar imagen
        image = Image.open(sample['image_path']).convert('RGB')
        
        # Crear prompts variados
        prompts = [
            f"a photo of {sample['city']}, {sample['state']}, Mexico",
            f"{sample['city']}, {sample['state']}",
            f"landscape of {sample['city']}, Mexico",
            f"{sample['city']} city view",
        ]
        
        # Agregar tags si existen
        if sample['tags']:
            tags = sample['tags'].split(',')
            for tag in tags[:3]:
                prompts.append(f"{tag.strip()} in {sample['city']}")
        
        # Procesar con CLIP
        inputs = self.processor(
            text=prompts,
            images=image,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        return {
            'pixel_values': inputs['pixel_values'].squeeze(0),
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask']
        }

# ============================================================================
# FINE-TUNING
# ============================================================================

class ContrastiveLoss(nn.Module):
    """P√©rdida contrastiva para CLIP"""
    
    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss()
    
    def forward(self, image_embeds, text_embeds):
        # Normalizar embeddings
        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)
        
        # Calcular similitud
        logits = torch.matmul(image_embeds, text_embeds.t()) / self.temperature
        
        # Labels (diagonal = positivos)
        batch_size = image_embeds.shape[0]
        labels = torch.arange(batch_size, device=logits.device)
        
        # P√©rdida bidireccional
        loss_i2t = self.criterion(logits, labels)
        loss_t2i = self.criterion(logits.t(), labels)
        
        return (loss_i2t + loss_t2i) / 2

def train_epoch(model, dataloader, optimizer, criterion, device):
    """Entrena una √©poca"""
    model.train()
    total_loss = 0
    
    for batch in tqdm(dataloader, desc="Entrenando"):
        pixel_values = batch['pixel_values'].to(device)
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        
        # Forward pass
        outputs = model(
            pixel_values=pixel_values,
            input_ids=input_ids.squeeze(1),
            attention_mask=attention_mask.squeeze(1)
        )
        
        # Calcular p√©rdida
        loss = criterion(outputs.image_embeds, outputs.text_embeds)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

def validate(model, dataloader, criterion, device):
    """Valida el modelo"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validando"):
            pixel_values = batch['pixel_values'].to(device)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            outputs = model(
                pixel_values=pixel_values,
                input_ids=input_ids.squeeze(1),
                attention_mask=attention_mask.squeeze(1)
            )
            
            loss = criterion(outputs.image_embeds, outputs.text_embeds)
            total_loss += loss.item()
    
    return total_loss / len(dataloader)

def finetune_model(epochs=5, batch_size=8, learning_rate=1e-5, min_quality=2, min_confidence=50):
    """Fine-tuning del modelo CLIP con interfaz Streamlit"""
    
    # Verificar anotaciones
    if not ANNOTATIONS_FILE.exists():
        st.error("‚ùå No hay anotaciones")
        return False
    
    with open(ANNOTATIONS_FILE, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    
    # Crear contenedor para logs
    log_container = st.container()
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Configurar dispositivo
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    log_container.info(f"üñ•Ô∏è Usando: {device}")
    
    # Cargar modelo y procesador
    status_text.text("üì¶ Cargando CLIP...")
    model_name = "openai/clip-vit-large-patch14"
    processor = CLIPProcessor.from_pretrained(model_name)
    model = CLIPModel.from_pretrained(model_name).to(device)
    progress_bar.progress(0.1)
    
    # Crear dataset
    status_text.text("üìä Creando dataset...")
    dataset = GeoDataset(annotations, processor, min_quality, min_confidence)
    
    if len(dataset) < 20:
        st.error(f"‚ùå Dataset muy peque√±o: {len(dataset)} im√°genes")
        st.info("üí° Ajusta los filtros o anota m√°s im√°genes")
        return False
    
    log_container.success(f"‚úÖ Dataset creado: {len(dataset)} im√°genes v√°lidas")
    progress_bar.progress(0.2)
    
    # Split train/val
    val_size = int(len(dataset) * 0.15)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    log_container.info(f"üìà Train: {len(train_dataset)} | Val: {len(val_dataset)}")
    
    # Configurar entrenamiento
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
    criterion = ContrastiveLoss()
    
    best_val_loss = float('inf')
    progress_bar.progress(0.3)
    
    # Contenedor para m√©tricas
    metrics_placeholder = st.empty()
    
    # Entrenamiento
    for epoch in range(epochs):
        status_text.text(f"üìç √âpoca {epoch+1}/{epochs}")
        
        # Simulaci√≥n de progreso (en producci√≥n usar√≠as callbacks reales)
        epoch_progress = 0.3 + (0.6 * (epoch + 1) / epochs)
        progress_bar.progress(epoch_progress)
        
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss = validate(model, val_loader, criterion, device)
        
        # Mostrar m√©tricas
        with metrics_placeholder.container():
            col1, col2 = st.columns(2)
            col1.metric(f"√âpoca {epoch+1} - Train Loss", f"{train_loss:.4f}")
            col2.metric(f"√âpoca {epoch+1} - Val Loss", f"{val_loss:.4f}")
        
        # Guardar checkpoint
        checkpoint_path = CHECKPOINT_DIR / f"checkpoint_epoch{epoch+1}.pth"
        torch.save(model.state_dict(), checkpoint_path)
        
        # Guardar mejor modelo
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_path = MODEL_DIR / "modelo_finetuned.pth"
            torch.save(model.state_dict(), best_model_path)
            log_container.success(f"‚≠ê Mejor modelo en √©poca {epoch+1} (val_loss: {val_loss:.4f})")
    
    progress_bar.progress(1.0)
    status_text.text("‚úÖ Fine-tuning completado")
    
    return True

# ============================================================================
# REGENERAR EMBEDDINGS
# ============================================================================

def build_model():
    """Regenera embeddings de ciudades con el modelo fine-tuned (versi√≥n Streamlit)"""
    
    finetuned_path = MODEL_DIR / "modelo_finetuned.pth"
    
    # Crear contenedores
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Configurar dispositivo
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    status_text.text(f"üñ•Ô∏è Usando: {device}")
    
    # Cargar modelo fine-tuned
    status_text.text("üì¶ Cargando modelo fine-tuned...")
    model_name = "openai/clip-vit-large-patch14"
    processor = CLIPProcessor.from_pretrained(model_name)
    model = CLIPModel.from_pretrained(model_name).to(device)
    model.load_state_dict(torch.load(finetuned_path, map_location=device, weights_only=False))
    model.eval()
    progress_bar.progress(0.2)
    
    # Cargar ciudades
    status_text.text("üìç Cargando ciudades...")
    cities = []
    with open(CITIES_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        cities = list(reader)
    
    st.success(f"‚úÖ {len(cities)} ciudades encontradas")
    progress_bar.progress(0.3)
    
    # Generar embeddings
    city_embeds = []
    status_text.text("üîÑ Generando embeddings...")
    
    with torch.no_grad():
        for idx, city in enumerate(cities):
            # Actualizar progreso
            progress = 0.3 + (0.6 * (idx + 1) / len(cities))
            progress_bar.progress(progress)
            status_text.text(f"üîÑ Procesando: {city['name']}, {city['state']} ({idx+1}/{len(cities)})")
            
            # Crear prompts m√∫ltiples
            prompts = [
                f"a photo of {city['name']}, {city['state']}, Mexico",
                f"{city['name']}, {city['state']}",
                f"landscape of {city['name']}, Mexico",
                f"tourist destination {city['name']}",
            ]
            
            # Agregar tags
            if city.get('tags'):
                tags = city['tags'].split(',')
                for tag in tags[:3]:
                    prompts.append(f"{tag.strip()} in {city['name']}")
            
            # Procesar textos
            inputs = processor(text=prompts, return_tensors="pt", padding=True, truncation=True).to(device)
            text_features = model.get_text_features(**inputs)
            
            # Promediar embeddings
            avg_embed = text_features.mean(dim=0)
            city_embeds.append(avg_embed.cpu().numpy())
    
    # Guardar modelo completo
    status_text.text("üíæ Guardando modelo...")
    output_path = MODEL_DIR / "modelo.pth"
    torch.save({
        'city_embeds': torch.tensor(city_embeds),
        'cities': cities,
        'model_name': model_name,
        'states': list(set(c['state'] for c in cities)),
        'state_embeds': {}  # Placeholder
    }, output_path)
    
    progress_bar.progress(1.0)
    status_text.text("‚úÖ Embeddings regenerados")
    
    st.success(f"üìÅ Modelo guardado en: {output_path}")
    st.info(f"üéØ {len(cities)} ciudades procesadas")

# ============================================================================
# EVALUACI√ìN DE MODELO
# ============================================================================

def show_evaluation_interface():
    """Interfaz para evaluar el modelo con im√°genes random"""
    
    st.header("üéØ Evaluaci√≥n del Modelo")
    st.markdown("Eval√∫a la precisi√≥n del modelo con im√°genes aleatorias.")
    
    # Verificar que exista modelo
    model_path = MODEL_DIR / "modelo.pth"
    if not model_path.exists():
        st.error("‚ùå No existe modelo entrenado")
        st.info("Primero debes ejecutar el **üî¨ Fine-tuning** y **üèóÔ∏è Regenerar Modelo**")
        return
    
    # Verificar anotaciones CSV
    if not ANNOTATIONS_CSV.exists():
        st.error("‚ùå No hay anotaciones en CSV")
        st.info("Primero anota im√°genes en el modo **üìù Anotaci√≥n**")
        return
    
    # Leer CSV de anotaciones
    import csv
    annotations_data = []
    with open(ANNOTATIONS_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        annotations_data = list(reader)
    
    if len(annotations_data) < 10:
        st.warning(f"‚ö†Ô∏è Solo hay {len(annotations_data)} anotaciones")
        st.info("Se recomienda tener al menos 20 im√°genes anotadas para una evaluaci√≥n significativa")
    
    st.success(f"‚úÖ {len(annotations_data)} im√°genes anotadas disponibles")
    
    # Configuraci√≥n de evaluaci√≥n
    st.subheader("‚öôÔ∏è Configuraci√≥n")
    
    col1, col2 = st.columns(2)
    with col1:
        num_samples = st.slider("N√∫mero de im√°genes a evaluar", 10, min(100, len(annotations_data)), min(20, len(annotations_data)))
    with col2:
        quality_filter = st.slider("Calidad m√≠nima", 1, 5, 3)
    
    if st.button("üöÄ Iniciar Evaluaci√≥n", type="primary", use_container_width=True):
        with st.spinner("Evaluando modelo..."):
            results = evaluate_model(annotations_data, num_samples, quality_filter)
            
            if results:
                st.divider()
                st.subheader("üìä Resultados de Evaluaci√≥n")
                
                # M√©tricas principales
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("üéØ Precisi√≥n Total", f"{results['accuracy']:.1f}%")
                col2.metric("‚úÖ Top-1 Accuracy", f"{results['top1_accuracy']:.1f}%")
                col3.metric("üîù Top-3 Accuracy", f"{results['top3_accuracy']:.1f}%")
                col4.metric("üìè Dist. Promedio", f"{results['avg_distance']:.1f} km")
                
                # Tabla de predicciones
                st.subheader("üîç Predicciones Detalladas")
                
                df_results = pd.DataFrame(results['predictions'])
                df_results['correct'] = df_results.apply(
                    lambda row: '‚úÖ' if row['predicted_city'] == row['true_city'] else '‚ùå',
                    axis=1
                )
                
                st.dataframe(
                    df_results[['filename', 'true_city', 'predicted_city', 'confidence', 'distance_km', 'correct']],
                    use_container_width=True,
                    hide_index=True
                )
                
                # An√°lisis por ciudad
                st.subheader("üèôÔ∏è Precisi√≥n por Ciudad")
                city_accuracy = {}
                for pred in results['predictions']:
                    city = pred['true_city']
                    if city not in city_accuracy:
                        city_accuracy[city] = {'correct': 0, 'total': 0}
                    city_accuracy[city]['total'] += 1
                    if pred['predicted_city'] == pred['true_city']:
                        city_accuracy[city]['correct'] += 1
                
                city_stats = []
                for city, stats in city_accuracy.items():
                    accuracy = (stats['correct'] / stats['total']) * 100
                    city_stats.append({
                        'Ciudad': city,
                        'Correctas': stats['correct'],
                        'Total': stats['total'],
                        'Precisi√≥n %': f"{accuracy:.1f}%"
                    })
                
                df_city = pd.DataFrame(city_stats).sort_values('Precisi√≥n %', ascending=False)
                st.dataframe(df_city, use_container_width=True, hide_index=True)
                
                # Gr√°fico de distribuci√≥n de distancias
                st.subheader("üìè Distribuci√≥n de Errores de Distancia")
                distances = [p['distance_km'] for p in results['predictions']]
                
                import matplotlib.pyplot as plt
                fig, ax = plt.subplots()
                ax.hist(distances, bins=20, color='skyblue', edgecolor='black')
                ax.set_xlabel('Distancia del error (km)')
                ax.set_ylabel('Frecuencia')
                ax.set_title('Distribuci√≥n de errores de distancia')
                st.pyplot(fig)

def evaluate_model(annotations_data, num_samples, quality_filter):
    """Eval√∫a el modelo con im√°genes aleatorias"""
    import random
    from math import radians, cos, sin, asin, sqrt
    
    # Filtrar por calidad
    filtered_data = [
        ann for ann in annotations_data 
        if int(ann.get('quality', 0)) >= quality_filter
    ]
    
    if len(filtered_data) < num_samples:
        st.error(f"‚ùå Solo hay {len(filtered_data)} im√°genes con calidad >= {quality_filter}")
        return None
    
    # Seleccionar muestras aleatorias
    samples = random.sample(filtered_data, num_samples)
    
    # Cargar modelo
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model_path = MODEL_DIR / "modelo.pth"
    model_data = torch.load(model_path, map_location=device, weights_only=False)
    
    city_embeds = model_data['city_embeds'].to(device)
    cities = model_data['cities']
    model_name = model_data['model_name']
    
    # Cargar CLIP
    processor = CLIPProcessor.from_pretrained(model_name)
    model = CLIPModel.from_pretrained(model_name).to(device)
    
    # Cargar modelo fine-tuned si existe
    finetuned_path = MODEL_DIR / "modelo_finetuned.pth"
    if finetuned_path.exists():
        model.load_state_dict(torch.load(finetuned_path, map_location=device, weights_only=False))
    
    model.eval()
    
    # Funci√≥n para calcular distancia haversine
    def haversine(lon1, lat1, lon2, lat2):
        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
        dlon = lon2 - lon1
        dlat = lat2 - lat1
        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
        c = 2 * asin(sqrt(a))
        km = 6371 * c
        return km
    
    # Evaluar cada muestra
    predictions = []
    correct_top1 = 0
    correct_top3 = 0
    total_distance = 0
    
    progress_bar = st.progress(0)
    
    with torch.no_grad():
        for idx, sample in enumerate(samples):
            # Actualizar progreso
            progress_bar.progress((idx + 1) / len(samples))
            
            # Cargar imagen
            img_path = IMAGES_DIR / sample['filename']
            if not img_path.exists():
                continue
            
            image = Image.open(img_path).convert('RGB')
            
            # Procesar imagen
            inputs = processor(images=image, return_tensors="pt").to(device)
            image_features = model.get_image_features(**inputs)
            
            # Calcular similitudes
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            city_embeds_norm = city_embeds / city_embeds.norm(dim=-1, keepdim=True)
            similarities = torch.matmul(image_features, city_embeds_norm.t()).cpu().numpy()[0]
            
            # Top-3 predicciones
            top3_indices = similarities.argsort()[-3:][::-1]
            
            predicted_city = cities[top3_indices[0]]['name']
            predicted_state = cities[top3_indices[0]]['state']
            confidence = float(similarities[top3_indices[0]])
            
            # Ciudad verdadera
            true_city = sample['city']
            true_state = sample['state']
            true_lat = float(sample['lat'])
            true_lon = float(sample['lon'])
            
            # Calcular distancia del error
            pred_lat = float(cities[top3_indices[0]]['lat'])
            pred_lon = float(cities[top3_indices[0]]['lon'])
            distance = haversine(true_lon, true_lat, pred_lon, pred_lat)
            
            # Verificar si es correcto
            is_correct_top1 = (predicted_city == true_city)
            is_correct_top3 = any(cities[idx]['name'] == true_city for idx in top3_indices)
            
            if is_correct_top1:
                correct_top1 += 1
            if is_correct_top3:
                correct_top3 += 1
            
            total_distance += distance
            
            predictions.append({
                'filename': sample['filename'],
                'true_city': f"{true_city}, {true_state}",
                'predicted_city': f"{predicted_city}, {predicted_state}",
                'confidence': f"{confidence:.3f}",
                'distance_km': round(distance, 2),
                'correct_top1': is_correct_top1
            })
    
    progress_bar.progress(1.0)
    
    # Calcular m√©tricas
    accuracy = (correct_top1 / len(predictions)) * 100
    top1_accuracy = (correct_top1 / len(predictions)) * 100
    top3_accuracy = (correct_top3 / len(predictions)) * 100
    avg_distance = total_distance / len(predictions)
    
    return {
        'accuracy': accuracy,
        'top1_accuracy': top1_accuracy,
        'top3_accuracy': top3_accuracy,
        'avg_distance': avg_distance,
        'predictions': predictions
    }

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    main_interface()
