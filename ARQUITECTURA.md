# ğŸ—ï¸ Arquitectura del Sistema - Geolocalizador OSINT

## ğŸ“‹ Tabla de Contenidos
1. [VisiÃ³n General](#visiÃ³n-general)
2. [Componentes Principales](#componentes-principales)
3. [Flujo de Datos](#flujo-de-datos)
4. [Base de Datos Supabase](#base-de-datos-supabase)
5. [Almacenamiento de ImÃ¡genes](#almacenamiento-de-imÃ¡genes)
6. [Pipeline de Entrenamiento](#pipeline-de-entrenamiento)
7. [Modelo de IA](#modelo-de-ia)

---

## ğŸ¯ VisiÃ³n General

Sistema de geolocalizaciÃ³n basado en IA que identifica ciudades mexicanas a partir de fotografÃ­as. Utiliza CLIP (Contrastive Language-Image Pre-training) fine-tuneado con imÃ¡genes anotadas manualmente.

### TecnologÃ­as Core
- **Backend**: Python 3.13
- **Base de Datos**: Supabase (PostgreSQL)
- **Almacenamiento**: Supabase Storage (bucket pÃºblico)
- **Modelo IA**: OpenAI CLIP (ViT-Large-Patch14)
- **Framework UI**: Streamlit
- **Deep Learning**: PyTorch + Transformers

---

## ğŸ§© Componentes Principales

### 1. **Mining Pipeline** (`mining_pipeline.py`)
**PropÃ³sito**: Descarga automÃ¡tica de imÃ¡genes de ciudades mexicanas desde mÃºltiples fuentes.

**Flujo de Trabajo**:
```
Ciudades CSV â†’ BÃºsqueda en APIs â†’ Descarga â†’ SanitizaciÃ³n â†’ Supabase Storage â†’ Base de Datos
```

**Fuentes de Datos**:
- **Wikimedia Commons**: Fotos de monumentos y lugares emblemÃ¡ticos
- **Wikipedia**: ImÃ¡genes de artÃ­culos de ciudades
- **Pexels**: FotografÃ­as profesionales de stock

**Proceso de Minado**:
1. Lee `data/cities_mx.csv` (105 ciudades mexicanas)
2. Para cada ciudad:
   - Busca imÃ¡genes en las 3 fuentes
   - Descarga hasta N imÃ¡genes por fuente (configurable)
   - Sanitiza nombres de archivo (lowercase, sin acentos, underscores)
   - Detecta duplicados con hash MD5
   - Sube a Supabase Storage
   - Guarda metadata en base de datos

**Formato de Nombres de Archivo**:
```
{source}_{city}_{state}_{index}_{timestamp}.jpg

Ejemplo:
pexels_guadalajara_jalisco_0_1764047289.jpg
wikimedia_ciudad_de_mexico_cdmx_5_1764045693.jpg
```

**Metadata Generada**:
- Filename, ciudad, estado, coordenadas (lat/lon)
- Fuente, URL original, tÃ­tulo, fotÃ³grafo
- Dimensiones, tamaÃ±o, hash MD5
- URL de Supabase Storage

### 2. **Training Pipeline** (`training_pipeline.py`)
**PropÃ³sito**: Interfaz Streamlit para anotaciÃ³n manual y fine-tuning del modelo.

**Modos de OperaciÃ³n**:

#### ğŸ“ **AnotaciÃ³n**
- Carga imÃ¡genes desde Supabase Storage (sin necesidad de archivos locales)
- Sistema de cola balanceada por estado (evita sesgo geogrÃ¡fico)
- Formulario de anotaciÃ³n con:
  - âœ… VerificaciÃ³n de ciudad correcta
  - â­ Calidad/relevancia (1-5 estrellas)
  - ğŸ·ï¸ Etiquetas personalizadas (tags libres)
  - ğŸ‘ï¸ Elementos detectados (landmarks, arquitectura, naturaleza, etc.)
  - ğŸ¯ Confianza del anotador (0-100%)
  - ğŸ“ Notas adicionales
- Opciones de eliminaciÃ³n de imÃ¡genes corruptas/irrelevantes
- Guarda en Supabase (`annotations` table) y CSV local (backup)

**Sistema de Balance**:
```python
# Algoritmo Round-Robin por estado
Jalisco: [img1, img2, img3, ...]
CDMX:    [img1, img2, img3, ...]
YucatÃ¡n: [img1, img2, ...]

Lista balanceada:
[Jalisco_img1, CDMX_img1, YucatÃ¡n_img1, 
 Jalisco_img2, CDMX_img2, YucatÃ¡n_img2, ...]
```

#### ğŸ”¬ **Fine-tuning**
- Entrena CLIP con imÃ¡genes anotadas (mÃ­nimo 50, recomendado 100+)
- Filtros configurables:
  - Calidad mÃ­nima (1-5)
  - Confianza mÃ­nima (0-100%)
- Arquitectura:
  - Loss: Contrastive Loss (temperatura 0.07)
  - Optimizador: AdamW
  - Split: 85% train / 15% validation
  - Early stopping en validation loss
- Guarda modelo fine-tuneado en `model/modelo_finetuned.pth`

#### ğŸ—ï¸ **Regenerar Embeddings**
- Carga modelo fine-tuneado
- Genera embeddings para todas las ciudades
- Guarda modelo completo en `model/modelo.pth`:
  ```python
  {
    'city_embeds': Tensor[105, 768],  # Embeddings de ciudades
    'cities': List[Dict],              # Metadata de ciudades
    'model_name': str,                 # "openai/clip-vit-large-patch14"
    'states': List[str],               # Estados Ãºnicos
  }
  ```

#### ğŸ“Š **EstadÃ­sticas**
- DistribuciÃ³n de calidad de anotaciones
- Top ciudades anotadas
- Elementos mÃ¡s comunes detectados
- Etiquetas personalizadas populares

#### ğŸ¯ **EvaluaciÃ³n**
- Prueba modelo con imÃ¡genes aleatorias anotadas
- MÃ©tricas:
  - Top-1 Accuracy (predicciÃ³n exacta)
  - Top-3 Accuracy (top 3 predicciones)
  - Distancia promedio (error en km)
- Muestra predicciones con confianza y distancia real

### 3. **Geolocalizador** (`Geolocalizador.py`)
**PropÃ³sito**: Interfaz de usuario final para geolocalizar imÃ¡genes.

**Funcionalidades**:
- Sube una foto â†’ Obtiene predicciÃ³n de ciudad
- Muestra top 5 predicciones con porcentajes de confianza
- VisualizaciÃ³n en mapa interactivo (Folium)
- InformaciÃ³n detallada de la ciudad predicha

**Proceso Interno**:
1. Usuario sube imagen
2. CLIP procesa imagen â†’ embedding de imagen
3. Compara con embeddings de 105 ciudades (cosine similarity)
4. Ordena por similitud
5. Retorna top 5 con porcentajes

### 4. **Supabase Client** (`supabase_client.py`)
**PropÃ³sito**: Wrapper de funciones para interactuar con Supabase.

**Funciones Principales**:
```python
# Lectura
get_all_images()              # Todas las imÃ¡genes
get_pending_images()          # Sin anotar
get_annotated_filenames()     # Nombres de anotadas
get_annotation_stats()        # EstadÃ­sticas agregadas

# Escritura
save_annotation(data)         # Guardar anotaciÃ³n
mark_deleted(filename, reason) # Marcar eliminada
```

### 5. **Upload to Storage** (`upload_to_supabase_storage.py`)
**PropÃ³sito**: Script de migraciÃ³n para subir imÃ¡genes locales a Supabase.

**Uso** (una sola vez):
```bash
python upload_to_supabase_storage.py
```
- Sube todas las imÃ¡genes de `data/mining/images/`
- Genera URLs pÃºblicas de Supabase Storage
- Actualiza `image_metadata.image_url` en BD

---

## ğŸ”„ Flujo de Datos

### Flujo Completo del Sistema

```
1. MINADO DE DATOS
   â”œâ”€ cities_mx.csv (105 ciudades)
   â”œâ”€ APIs (Wikimedia, Wikipedia, Pexels)
   â”œâ”€ Descarga local temporal
   â”œâ”€ Supabase Storage upload
   â””â”€ Base de datos (image_metadata)

2. ANOTACIÃ“N
   â”œâ”€ Streamlit carga desde Supabase
   â”œâ”€ Usuario anota imagen
   â”œâ”€ Guarda en annotations table
   â””â”€ CSV backup local

3. ENTRENAMIENTO
   â”œâ”€ Lee annotations desde Supabase
   â”œâ”€ Fine-tune CLIP
   â”œâ”€ Genera modelo_finetuned.pth
   â””â”€ Regenera embeddings â†’ modelo.pth

4. PREDICCIÃ“N
   â”œâ”€ Usuario sube foto
   â”œâ”€ CLIP genera embedding
   â”œâ”€ Compara con ciudad_embeds
   â””â”€ Retorna top 5 ciudades
```

### Flujo de Imagen Individual

```
API â†’ Download â†’ Sanitize â†’ Storage â†’ Database â†’ Annotation â†’ Training â†’ Model
 â”‚                   â†“                    â†“           â†“           â†“         â†“
Photo            filename.jpg         image_url   quality=4   Fine-tune  Predict
                                     (pÃºblico)    tags=[...]   CLIP      City
```

---

## ğŸ—„ï¸ Base de Datos Supabase

### Esquema de Tablas

#### **image_metadata**
```sql
CREATE TABLE image_metadata (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename TEXT NOT NULL UNIQUE,
    image_url TEXT,                    -- URL pÃºblica de Supabase Storage
    city TEXT NOT NULL,
    state TEXT NOT NULL,
    lat DECIMAL(10, 8),
    lon DECIMAL(11, 8),
    source TEXT,                       -- 'pexels', 'wikimedia', 'wikipedia'
    photo_id TEXT,
    url TEXT,                          -- URL original de la fuente
    title TEXT,
    photographer TEXT,
    width INTEGER,
    height INTEGER,
    size INTEGER,
    hash TEXT,                         -- MD5 para detectar duplicados
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### **annotations**
```sql
CREATE TABLE annotations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    image_id UUID REFERENCES image_metadata(id) UNIQUE,
    correct_city TEXT,                 -- ConfirmaciÃ³n de ciudad
    quality INTEGER CHECK (quality BETWEEN 1 AND 5),
    confidence INTEGER CHECK (confidence BETWEEN 0 AND 100),
    
    -- Elementos detectados (9 banderas booleanas)
    has_landmarks BOOLEAN DEFAULT FALSE,
    has_architecture BOOLEAN DEFAULT FALSE,
    has_nature BOOLEAN DEFAULT FALSE,
    has_urban BOOLEAN DEFAULT FALSE,
    has_cultural BOOLEAN DEFAULT FALSE,
    has_religious BOOLEAN DEFAULT FALSE,
    has_modern BOOLEAN DEFAULT FALSE,
    has_historical BOOLEAN DEFAULT FALSE,
    has_coastal BOOLEAN DEFAULT FALSE,
    
    custom_tags TEXT[],               -- Array de PostgreSQL
    notes TEXT,
    annotated_by TEXT,                -- Nombre del anotador
    annotated_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

#### **deleted_images**
```sql
CREATE TABLE deleted_images (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename TEXT NOT NULL UNIQUE,
    reason TEXT,                      -- Motivo de eliminaciÃ³n
    deleted_by TEXT,
    deleted_at TIMESTAMP DEFAULT NOW()
);
```

### Vistas Agregadas

#### **pending_images**
```sql
CREATE VIEW pending_images AS
SELECT im.*
FROM image_metadata im
LEFT JOIN annotations a ON im.id = a.image_id
WHERE a.id IS NULL;
```

#### **annotated_images**
```sql
CREATE VIEW annotated_images AS
SELECT 
    im.*,
    a.quality,
    a.confidence,
    a.custom_tags,
    a.annotated_by,
    a.annotated_at
FROM image_metadata im
INNER JOIN annotations a ON im.id = a.image_id;
```

#### **annotation_stats**
```sql
CREATE VIEW annotation_stats AS
SELECT 
    COUNT(DISTINCT im.id) as total_images,
    COUNT(DISTINCT a.id) as annotated_count,
    COUNT(DISTINCT im.state) as unique_states,
    COUNT(DISTINCT CASE WHEN a.id IS NULL THEN im.id END) as pending_count
FROM image_metadata im
LEFT JOIN annotations a ON im.id = a.image_id;
```

### Triggers

```sql
-- Auto-actualizar updated_at
CREATE TRIGGER update_image_metadata_updated_at
    BEFORE UPDATE ON image_metadata
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_annotations_updated_at
    BEFORE UPDATE ON annotations
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

---

## ğŸ“¦ Almacenamiento de ImÃ¡genes

### Supabase Storage

**Bucket**: `geolocalization-images` (pÃºblico)

**PolÃ­ticas RLS**:
```sql
-- Permitir lectura pÃºblica
CREATE POLICY "Lectura pÃºblica"
ON storage.objects FOR SELECT
TO public
USING (bucket_id = 'geolocalization-images');

-- Permitir subida pÃºblica
CREATE POLICY "Subida pÃºblica"
ON storage.objects FOR INSERT
TO public
WITH CHECK (bucket_id = 'geolocalization-images');
```

**Estructura de URLs**:
```
https://qlwzmjyztyfnhoxfjstd.supabase.co/storage/v1/object/public/geolocalization-images/{filename}

Ejemplo:
https://qlwzmjyztyfnhoxfjstd.supabase.co/storage/v1/object/public/geolocalization-images/pexels_guadalajara_jalisco_0_1764047289.jpg
```

**Ventajas**:
- âœ… Acceso pÃºblico sin autenticaciÃ³n
- âœ… CDN integrado para carga rÃ¡pida
- âœ… No requiere archivos locales en producciÃ³n
- âœ… Deployment simplificado (Streamlit Cloud)
- âœ… URLs permanentes para referencias

---

## ğŸ“ Pipeline de Entrenamiento

### Arquitectura de Fine-tuning

```
Modelo Base: openai/clip-vit-large-patch14
â”œâ”€ Vision Transformer (ViT-Large)
â”‚  â”œâ”€ Patch size: 14x14
â”‚  â”œâ”€ Hidden size: 1024
â”‚  â””â”€ Embedding dim: 768
â””â”€ Text Transformer
   â”œâ”€ Vocabulary: 49408 tokens
   â””â”€ Embedding dim: 768
```

### Dataset Personalizado

```python
class GeoDataset(Dataset):
    """
    Carga imÃ¡genes anotadas y genera pares imagen-texto
    
    Filtros:
    - min_quality: 1-5 (default: 2)
    - min_confidence: 0-100% (default: 50%)
    
    Texto generado:
    "A photo of {city}, {state}. Tags: {custom_tags}. 
     Elements: {detected_elements}"
    """
```

### Loss Function

```python
class ContrastiveLoss(nn.Module):
    """
    PÃ©rdida contrastiva simÃ©trica
    
    - Normaliza embeddings de imagen y texto
    - Calcula similitud coseno
    - Aplica temperatura (0.07)
    - Cross-entropy bidireccional
    """
```

### Proceso de Entrenamiento

1. **PreparaciÃ³n**:
   - Filtra anotaciones por calidad y confianza
   - Split 85% train / 15% validation
   - Batch size: 8 (configurable)

2. **Training Loop**:
   ```python
   for epoch in range(epochs):
       # Forward pass
       image_embeds = model.get_image_features(images)
       text_embeds = model.get_text_features(texts)
       
       # Contrastive loss
       loss = criterion(image_embeds, text_embeds)
       
       # Backward pass
       optimizer.zero_grad()
       loss.backward()
       optimizer.step()
   ```

3. **Validation**:
   - Calcula loss en validation set
   - Early stopping si no mejora

4. **Guardado**:
   - Mejor modelo â†’ `modelo_finetuned.pth`
   - Incluye solo `state_dict`

### GeneraciÃ³n de Embeddings

```python
def build_model():
    """
    1. Carga modelo fine-tuneado
    2. Para cada ciudad:
       - Genera texto: "A photo of {city}, {state}"
       - Obtiene text embedding
       - Almacena en tensor
    3. Guarda modelo completo con embeddings
    """
```

---

## ğŸ¤– Modelo de IA

### CLIP (Contrastive Language-Image Pre-training)

**Paper**: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

**Arquitectura**:
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   IMAGEN    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ViT Encoder    â”‚
                    â”‚  (Vision)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Image Embedding â”‚ â”€â”€â”
                    â”‚    (768 dim)    â”‚   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                          â”‚ Cosine
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ Similarity
                    â”‚    TEXTO    â”‚       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â”‚
                           â”‚              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚ Text Encoder    â”‚   â”‚
                    â”‚ (Transformer)   â”‚   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                           â”‚              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚ Text Embedding  â”‚ â”€â”€â”˜
                    â”‚   (768 dim)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Proceso de PredicciÃ³n

```python
def predict(image_path, model_path):
    # 1. Cargar modelo y embeddings
    model_data = torch.load(model_path)
    city_embeds = model_data['city_embeds']  # [105, 768]
    cities = model_data['cities']
    
    # 2. Procesar imagen
    image = Image.open(image_path)
    inputs = processor(images=image, return_tensors="pt")
    
    # 3. Generar embedding de imagen
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
        image_embed = image_features / image_features.norm(dim=-1, keepdim=True)
    
    # 4. Calcular similitudes
    similarities = (image_embed @ city_embeds.T).squeeze(0)
    
    # 5. Top 5 predicciones
    top5_indices = similarities.argsort(descending=True)[:5]
    top5_cities = [cities[i] for i in top5_indices]
    top5_scores = [similarities[i].item() for i in top5_indices]
    
    return top5_cities, top5_scores
```

### Mejoras del Fine-tuning

**Antes** (modelo base):
- Entrenado con textos genÃ©ricos de internet
- No conoce nombres de ciudades mexicanas
- No entiende caracterÃ­sticas arquitectÃ³nicas locales

**DespuÃ©s** (fine-tuned):
- Aprende patrones visuales de ciudades mexicanas
- Asocia landmarks especÃ­ficos con ciudades
- Entiende contexto cultural y arquitectÃ³nico
- Mejora significativa en top-1 accuracy

---

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### Top-1 Accuracy
```
PredicciÃ³n exacta de la ciudad correcta
Ejemplo: Predice "Guadalajara" y es "Guadalajara" âœ…
```

### Top-3 Accuracy
```
Ciudad correcta estÃ¡ en las 3 primeras predicciones
Ejemplo: Top 3 = ["Zapopan", "Guadalajara", "Tlaquepaque"]
         Real = "Guadalajara" âœ…
```

### Distancia Promedio
```
Distancia haversine entre predicciÃ³n y realidad (en km)

Formula:
a = sinÂ²(Î”lat/2) + cos(lat1) Ã— cos(lat2) Ã— sinÂ²(Î”lon/2)
c = 2 Ã— atan2(âˆša, âˆš(1âˆ’a))
d = R Ã— c  (R = 6371 km)
```

---

## ğŸš€ Deployment

### Streamlit Cloud

**Variables de Entorno** (`.streamlit/secrets.toml`):
```toml
SUPABASE_URL = "https://qlwzmjyztyfnhoxfjstd.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

**Ventajas**:
- No requiere archivos locales de imÃ¡genes
- Todo se carga desde Supabase Storage
- Deployment automÃ¡tico desde GitHub
- ColaboraciÃ³n en tiempo real

### Flujo de Deployment

```
1. GitHub Push
   â”œâ”€ .env â†’ ignorado (.gitignore)
   â”œâ”€ CÃ³digo â†’ main branch
   â””â”€ modelo.pth â†’ Git LFS

2. Streamlit Cloud
   â”œâ”€ Conecta a repo
   â”œâ”€ Configura secrets
   â”œâ”€ Auto-deploy
   â””â”€ URL pÃºblica

3. Supabase
   â”œâ”€ Base de datos PostgreSQL
   â”œâ”€ Storage (imÃ¡genes)
   â””â”€ Auto-sync con Streamlit
```

---

## ğŸ” Seguridad

### Row Level Security (RLS)

**Tablas de Datos**: RLS deshabilitado para desarrollo
```sql
ALTER TABLE image_metadata DISABLE ROW LEVEL SECURITY;
ALTER TABLE annotations DISABLE ROW LEVEL SECURITY;
```

**Storage**: PolÃ­ticas pÃºblicas para lectura/escritura
```sql
-- ProducciÃ³n: aÃ±adir autenticaciÃ³n
CREATE POLICY "Authenticated uploads"
ON storage.objects FOR INSERT
TO authenticated
WITH CHECK (bucket_id = 'geolocalization-images');
```

### Variables Sensibles

**.gitignore**:
```
.env
.streamlit/secrets.toml
token.pickle
*.pth (excepto via Git LFS)
```

---

## ğŸ“ˆ Roadmap Futuro

### Mejoras Planificadas

1. **MÃ¡s Datos**:
   - 500+ imÃ¡genes por ciudad
   - Diversidad de Ã¡ngulos, clima, Ã©poca del aÃ±o
   - AumentaciÃ³n de datos (rotaciÃ³n, crop, color)

2. **Modelo Mejorado**:
   - Ensemble de modelos (CLIP + ResNet + EfficientNet)
   - AtenciÃ³n espacial (destacar landmarks)
   - Transfer learning progresivo

3. **Features Adicionales**:
   - PredicciÃ³n de estado (32 estados)
   - DetecciÃ³n de landmarks especÃ­ficos
   - EstimaciÃ³n de coordenadas precisas
   - Reconocimiento de Ã©poca histÃ³rica

4. **OptimizaciÃ³n**:
   - CuantizaciÃ³n de modelo (reducir tamaÃ±o)
   - Caching de embeddings
   - API REST para integraciÃ³n externa

---

## ğŸ› ï¸ Mantenimiento

### Scripts de Utilidad

**MigraciÃ³n inicial** (ejecutar una vez):
```bash
# 1. Subir imÃ¡genes locales a Supabase
python upload_to_supabase_storage.py

# 2. Migrar metadata y anotaciones
python migrate_to_supabase.py
```

**SanitizaciÃ³n de nombres**:
```bash
# Limpiar nombres de archivos (acentos, espacios)
python rename_images.py
```

**ValidaciÃ³n de estructura**:
```bash
# Verificar integridad de datos
python validate_structure.py
```

### Backup

**Base de Datos**: Supabase hace backups automÃ¡ticos diarios

**ImÃ¡genes**: Almacenadas en Supabase Storage (redundante)

**Modelo**: 
```bash
# Guardar en Git LFS
git lfs track "*.pth"
git add model/modelo.pth
git commit -m "Update model"
```

---

## ğŸ“š Referencias

- [CLIP Paper](https://arxiv.org/abs/2103.00020)
- [Supabase Docs](https://supabase.com/docs)
- [Streamlit Docs](https://docs.streamlit.io)
- [PyTorch Docs](https://pytorch.org/docs)
- [Transformers Docs](https://huggingface.co/docs/transformers)
