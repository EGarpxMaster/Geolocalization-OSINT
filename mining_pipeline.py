"""
PIPELINE DE MINER√çA DE IM√ÅGENES - GEOLOCALIZATION OSINT
========================================================
Sistema unificado para miner√≠a de im√°genes desde fuentes abiertas.
Incluye: Wikimedia Commons, Wikipedia, Pexels (100% gratuito)

Uso:
    python mining_pipeline.py --mode all --images 20
    python mining_pipeline.py --mode state --state "Jalisco" --images 10
    python mining_pipeline.py --check-progress
"""

import os
import json
import hashlib
import requests
import csv
import time
import unicodedata
from datetime import datetime
from pathlib import Path
from collections import defaultdict
from urllib.parse import quote
from bs4 import BeautifulSoup
import argparse
from dotenv import load_dotenv
from supabase import create_client, Client

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
MINING_DIR = DATA_DIR / "mining"
IMAGES_DIR = MINING_DIR / "images"
METADATA_FILE = MINING_DIR / "metadata.json"
METADATA_CSV = MINING_DIR / "metadata.csv"
CITIES_CSV = DATA_DIR / "cities_mx.csv"

# Crear directorios
IMAGES_DIR.mkdir(parents=True, exist_ok=True)

# Configurar Supabase
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if SUPABASE_URL and SUPABASE_KEY:
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    USE_SUPABASE = True
    BUCKET_NAME = "geolocalization-images"
else:
    USE_SUPABASE = False
    print("‚ö†Ô∏è Supabase no configurado. Solo se guardar√°n archivos locales.")

# APIs gratuitas
PEXELS_API_KEY = "uaPqaLWpEWGCmQvywCUm7zTeWZrZJQFKkLdtRnxU4WhEXUS4zer3heNK"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ============================================================================
# UTILIDADES
# ============================================================================

def sanitize_name(text):
    """Sanitiza nombres de archivo: lowercase, sin acentos, underscores"""
    # Normalizar unicode (descomponer acentos)
    text = unicodedata.normalize('NFD', text)
    # Filtrar solo ASCII
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    # Reemplazos manuales adicionales
    replacements = {
        '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
        '√Å': 'a', '√â': 'e', '√ç': 'i', '√ì': 'o', '√ö': 'u',
        '√±': 'n', '√ë': 'n', ' ': '_', '-': '_'
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    # Lowercase y limpiar caracteres no v√°lidos
    text = ''.join(c if c.isalnum() or c == '_' else '_' for c in text)
    # Limpiar m√∫ltiples underscores
    while '__' in text:
        text = text.replace('__', '_')
    return text.lower().strip('_')

# ============================================================================
# FUNCIONES DE MINER√çA
# ============================================================================

def load_metadata():
    """Carga metadata existente"""
    if METADATA_FILE.exists():
        with open(METADATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"images": [], "cities": {}}

def save_metadata(metadata):
    """Guarda metadata en JSON y CSV"""
    try:
        # Guardar JSON (original)
        with open(METADATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        # Guardar CSV (nuevo - m√°s f√°cil de usar)
        save_metadata_csv(metadata)
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando metadata: {e}")

def save_metadata_csv(metadata):
    """Guarda metadata en formato CSV"""
    try:
        with open(METADATA_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # Encabezado
            writer.writerow([
                'filename', 'source', 'photo_id', 'city', 'state', 
                'lat', 'lon', 'url', 'title', 'photographer',
                'downloaded_at', 'size', 'hash'
            ])
            
            # Escribir cada imagen
            for img in metadata.get('images', []):
                # Obtener filename del campo correcto
                filename = img.get('filename', '')
                if not filename and 'local_path' in img:
                    filename = Path(img['local_path']).name
                
                writer.writerow([
                    filename,
                    img.get('source', ''),
                    img.get('photo_id', ''),
                    img.get('city', ''),  # Cambiado de city_target a city
                    img.get('state', ''),  # Cambiado de state_target a state
                    img.get('lat', 0.0),
                    img.get('lon', 0.0),
                    img.get('url', ''),
                    img.get('title', ''),
                    img.get('photographer', ''),
                    img.get('downloaded_at', ''),
                    img.get('size', 0),
                    img.get('hash', '')
                ])
        
        print(f"‚úÖ Metadata CSV guardado: {METADATA_CSV}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando CSV: {e}")

def get_image_hash(image_data):
    """Genera hash √∫nico para deduplicaci√≥n"""
    return hashlib.md5(image_data).hexdigest()

def download_image(url, save_path, metadata_entry):
    """Descarga imagen con validaci√≥n y sube a Supabase Storage"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        # Validar que es una imagen
        content_type = response.headers.get('content-type', '')
        if 'image' not in content_type.lower():
            return False
        
        # Validar tama√±o (min 50KB, max 10MB)
        size = len(response.content)
        if size < 50000 or size > 10000000:
            return False
        
        # Guardar imagen localmente (temporal)
        with open(save_path, 'wb') as f:
            f.write(response.content)
        
        metadata_entry['hash'] = get_image_hash(response.content)
        metadata_entry['size'] = size
        metadata_entry['downloaded_at'] = datetime.now().isoformat()
        
        # Subir a Supabase Storage si est√° configurado
        if USE_SUPABASE:
            try:
                filename = save_path.name
                
                # Subir archivo
                with open(save_path, 'rb') as f:
                    supabase.storage.from_(BUCKET_NAME).upload(
                        filename,
                        f.read(),
                        file_options={"content-type": "image/jpeg", "upsert": "true"}
                    )
                
                # Obtener URL p√∫blica
                image_url = supabase.storage.from_(BUCKET_NAME).get_public_url(filename)
                metadata_entry['image_url'] = image_url
                
                # Guardar metadata en base de datos
                supabase.table('image_metadata').upsert({
                    'filename': filename,
                    'image_url': image_url,
                    'city': metadata_entry.get('city'),
                    'state': metadata_entry.get('state'),
                    'lat': metadata_entry.get('lat'),
                    'lon': metadata_entry.get('lon'),
                    'source': metadata_entry.get('source'),
                    'photo_id': metadata_entry.get('photo_id'),
                    'url': metadata_entry.get('url'),
                    'title': metadata_entry.get('title'),
                    'photographer': metadata_entry.get('photographer'),
                    'width': metadata_entry.get('width', 0),
                    'height': metadata_entry.get('height', 0),
                    'size': size,
                    'hash': metadata_entry['hash']
                }, on_conflict='filename').execute()
                
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Error subiendo a Supabase: {e}")
                # Continuar aunque falle Supabase, archivo local guardado
        
        return True
    except Exception as e:
        print(f"    ‚ùå Error descargando: {e}")
        return False

# ============================================================================
# FUENTES DE DATOS GRATUITAS
# ============================================================================

def search_wikimedia_commons(query, limit=10):
    """Busca im√°genes en Wikimedia Commons (100% gratuito, sin l√≠mites)"""
    results = []
    try:
        url = "https://commons.wikimedia.org/w/api.php"
        params = {
            'action': 'query',
            'format': 'json',
            'generator': 'search',
            'gsrnamespace': 6,  # File namespace
            'gsrsearch': f'{query} Mexico',
            'gsrlimit': limit,
            'prop': 'imageinfo',
            'iiprop': 'url|size|extmetadata',
            'iiurlwidth': 1024
        }
        
        response = requests.get(url, params=params, timeout=10, headers=HEADERS)
        response.raise_for_status()
        
        if not response.text.strip():
            return results
        
        data = response.json()
        
        if 'query' in data and 'pages' in data['query']:
            for page in data['query']['pages'].values():
                if 'imageinfo' in page:
                    info = page['imageinfo'][0]
                    results.append({
                        'url': info.get('url'),
                        'thumbnail': info.get('thumburl', info.get('url')),
                        'source': 'wikimedia',
                        'title': page.get('title', ''),
                        'width': info.get('width', 0),
                        'height': info.get('height', 0)
                    })
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Wikimedia error: {e}")
    
    return results

def search_wikipedia_images(query, limit=10):
    """Busca im√°genes en art√≠culos de Wikipedia"""
    results = []
    try:
        # Buscar art√≠culos relacionados
        search_url = f"https://es.wikipedia.org/w/api.php"
        params = {
            'action': 'query',
            'format': 'json',
            'list': 'search',
            'srsearch': f'{query} M√©xico',
            'srlimit': 3
        }
        
        response = requests.get(search_url, params=params, timeout=10, headers=HEADERS)
        response.raise_for_status()
        
        if not response.text.strip():
            return results
        
        data = response.json()
        
        if 'query' in data and 'search' in data['query']:
            for article in data['query']['search'][:2]:
                title = article['title']
                
                # Obtener im√°genes del art√≠culo
                img_params = {
                    'action': 'query',
                    'format': 'json',
                    'titles': title,
                    'prop': 'images',
                    'imlimit': limit
                }
                
                img_response = requests.get(search_url, params=img_params, timeout=10)
                img_data = img_response.json()
                
                if 'query' in img_data and 'pages' in img_data['query']:
                    for page in img_data['query']['pages'].values():
                        if 'images' in page:
                            for img in page['images'][:limit]:
                                img_title = img['title']
                                
                                # Obtener URL de la imagen
                                url_params = {
                                    'action': 'query',
                                    'format': 'json',
                                    'titles': img_title,
                                    'prop': 'imageinfo',
                                    'iiprop': 'url|size',
                                    'iiurlwidth': 1024
                                }
                                
                                url_response = requests.get(search_url, params=url_params, timeout=10)
                                url_data = url_response.json()
                                
                                if 'query' in url_data and 'pages' in url_data['query']:
                                    for img_page in url_data['query']['pages'].values():
                                        if 'imageinfo' in img_page:
                                            info = img_page['imageinfo'][0]
                                            results.append({
                                                'url': info.get('url'),
                                                'thumbnail': info.get('thumburl', info.get('url')),
                                                'source': 'wikipedia',
                                                'title': img_title,
                                                'width': info.get('width', 0),
                                                'height': info.get('height', 0)
                                            })
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Wikipedia error: {e}")
    
    return results[:limit]

def search_pexels(query, limit=10):
    """Busca im√°genes en Pexels (API gratuita con registro)"""
    results = []
    try:
        url = "https://api.pexels.com/v1/search"
        headers = {'Authorization': PEXELS_API_KEY}
        params = {
            'query': f'{query} Mexico',
            'per_page': min(limit, 80),
            'orientation': 'landscape'
        }
        
        response = requests.get(url, headers=headers, params=params, timeout=15)
        response.raise_for_status()
        
        if response.status_code == 200:
            data = response.json()
            for photo in data.get('photos', []):
                results.append({
                    'url': photo['src']['large2x'],
                    'thumbnail': photo['src']['medium'],
                    'source': 'pexels',
                    'title': f"Pexels {photo['id']}",
                    'photographer': photo.get('photographer', ''),
                    'width': photo.get('width', 0),
                    'height': photo.get('height', 0)
                })
    except requests.exceptions.RequestException as e:
        print(f"    ‚ö†Ô∏è  Pexels error: {e}")
    except Exception as e:
        print(f"    ‚ö†Ô∏è  Pexels error inesperado: {e}")
    
    return results

# ============================================================================
# PIPELINE DE MINER√çA
# ============================================================================

def mine_city(city_name, state, lat, lon, images_per_source=20):
    """Mina im√°genes de una ciudad desde todas las fuentes"""
    print(f"\n[{city_name}, {state}]")
    
    metadata = load_metadata()
    
    # Asegurar que exista la estructura base
    if 'cities' not in metadata:
        metadata['cities'] = {}
    if 'images' not in metadata:
        metadata['images'] = []
    
    # Sanitizar nombres para el formato de archivo
    city_sanitized = sanitize_name(city_name)
    state_sanitized = sanitize_name(state)
    
    city_key = f"{city_name}_{state}"
    
    if city_key not in metadata['cities']:
        metadata['cities'][city_key] = {
            'name': city_name,
            'state': state,
            'lat': lat,
            'lon': lon,
            'images': []
        }
    
    existing_hashes = {img.get('hash') for img in metadata['images']}
    new_images = 0
    
    # Buscar en cada fuente
    sources = [
        ('Wikimedia Commons', lambda: search_wikimedia_commons(city_name, images_per_source)),
        ('Wikipedia', lambda: search_wikipedia_images(city_name, images_per_source)),
        ('Pexels', lambda: search_pexels(city_name, images_per_source))
    ]
    
    for source_name, search_func in sources:
        print(f"  ‚Üí Buscando en {source_name}...")
        results = search_func()
        
        for idx, img in enumerate(results):
            if new_images >= images_per_source:
                break
            
            # Crear nombre de archivo √∫nico con nombres sanitizados
            source_prefix = img['source']
            filename = f"{source_prefix}_{city_sanitized}_{state_sanitized}_{idx}_{int(time.time())}.jpg"
            save_path = IMAGES_DIR / filename
            
            print(f"    ‚¨áÔ∏è  Descargando {source_prefix}/{img['title'][:30]}...", end=' ')
            
            metadata_entry = {
                'filename': filename,
                'city': city_name,
                'state': state,
                'lat': lat,
                'lon': lon,
                'source': img['source'],
                'photo_id': str(img.get('id', '')),
                'url': img['url'],
                'title': img.get('title', ''),
                'photographer': img.get('photographer', ''),
                'width': img.get('width', 0),
                'height': img.get('height', 0)
            }
            
            if download_image(img['url'], save_path, metadata_entry):
                # Verificar duplicados
                if metadata_entry['hash'] in existing_hashes:
                    print("‚ö†Ô∏è  Duplicado")
                    save_path.unlink()
                    continue
                
                metadata['images'].append(metadata_entry)
                metadata['cities'][city_key]['images'].append(filename)
                existing_hashes.add(metadata_entry['hash'])
                new_images += 1
                print("‚úÖ")
            else:
                print("‚ùå")
            
            time.sleep(0.5)  # Rate limiting
    
    save_metadata(metadata)
    print(f"  ‚úÖ Total descargadas: {new_images} im√°genes")
    return new_images

# ============================================================================
# FUNCIONES DE GESTI√ìN
# ============================================================================

def load_cities():
    """Carga todas las ciudades del CSV"""
    cities = []
    with open(CITIES_CSV, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cities.append({
                'name': row['name'],
                'state': row['state'],
                'lat': float(row['lat']),
                'lon': float(row['lon']),
                'tags': row.get('tags', '')
            })
    return cities

def get_cities_by_state(state_name=None):
    """Obtiene ciudades agrupadas por estado"""
    cities = load_cities()
    if state_name:
        return [c for c in cities if c['state'].lower() == state_name.lower()]
    
    states_dict = defaultdict(list)
    for city in cities:
        states_dict[city['state']].append(city)
    return dict(states_dict)

def mine_all_cities(images_per_city=20):
    """Mina todas las ciudades del CSV"""
    cities = load_cities()
    states_dict = get_cities_by_state()
    
    print("="*70)
    print("üó∫Ô∏è  MINER√çA COMPLETA - TODAS LAS CIUDADES DE M√âXICO")
    print("="*70)
    print(f"\n‚úÖ {len(cities)} ciudades identificadas")
    print(f"üó∫Ô∏è  {len(states_dict)} estados cubiertos\n")
    
    # Mostrar distribuci√≥n
    print("üìä DISTRIBUCI√ìN POR ESTADO:")
    for state, cities_list in sorted(states_dict.items()):
        total_imgs = len(cities_list) * images_per_city
        print(f"  ‚Ä¢ {state}: {len(cities_list)} ciudad(es) √ó {images_per_city} = {total_imgs} im√°genes")
    
    total_images = len(cities) * images_per_city
    print(f"\nüéØ TOTAL: {len(cities)} ciudades √ó {images_per_city} = {total_images} im√°genes")
    print(f"‚è±Ô∏è  Tiempo estimado: {total_images * 0.045:.0f}-{total_images * 0.06:.0f} minutos\n")
    
    print("Iniciando en 3 segundos...")
    time.sleep(3)
    
    start_time = time.time()
    total_downloaded = 0
    
    for idx, city in enumerate(cities, 1):
        print(f"\n{'='*70}")
        print(f"[{idx}/{len(cities)}] {city['name']}, {city['state']}")
        print(f"{'='*70}")
        
        downloaded = mine_city(
            city['name'],
            city['state'],
            city['lat'],
            city['lon'],
            images_per_city
        )
        total_downloaded += downloaded
        
        # Auto-save cada 5 ciudades
        if idx % 5 == 0:
            print(f"\nüíæ Checkpoint: {idx}/{len(cities)} ciudades completadas")
            print(f"üìä Progreso: {total_downloaded} im√°genes descargadas")
    
    elapsed = time.time() - start_time
    
    # Estad√≠sticas finales
    print("\n" + "="*70)
    print("‚úÖ MINER√çA COMPLETADA")
    print("="*70)
    print(f"‚è±Ô∏è  Tiempo total: {elapsed/60:.1f} minutos")
    print(f"üìä Im√°genes descargadas: {total_downloaded}")
    print(f"üó∫Ô∏è  Ciudades procesadas: {len(cities)}")
    print(f"üìÅ Ubicaci√≥n: {IMAGES_DIR}")
    print("\nüí° Siguiente paso: python training_pipeline.py --annotate")

def check_progress():
    """Verifica el progreso de la miner√≠a"""
    metadata = load_metadata()
    
    print("\n" + "="*70)
    print("üìä PROGRESO DE MINER√çA")
    print("="*70)
    
    total_images = len(metadata['images'])
    cities_count = len(metadata['cities'])
    
    if total_images == 0:
        print("\n‚ö†Ô∏è  No se han descargado im√°genes a√∫n")
        print("üí° Ejecuta: python mining_pipeline.py --mode all")
        return
    
    print(f"\n‚úÖ Total de im√°genes: {total_images}")
    print(f"üèôÔ∏è  Ciudades cubiertas: {cities_count}")
    
    # Contar por estado
    states = defaultdict(int)
    for city_data in metadata['cities'].values():
        states[city_data['state']] += len(city_data['images'])
    
    print(f"üó∫Ô∏è  Estados cubiertos: {len(states)}/32")
    
    # Top ciudades
    print("\nüìà Top 10 ciudades:")
    sorted_cities = sorted(
        metadata['cities'].items(),
        key=lambda x: len(x[1]['images']),
        reverse=True
    )
    for idx, (city_key, city_data) in enumerate(sorted_cities[:10], 1):
        print(f"  {idx}. {city_data['name']}, {city_data['state']}: {len(city_data['images'])} im√°genes")
    
    # Por fuente
    sources = defaultdict(int)
    for img in metadata['images']:
        sources[img['source']] += 1
    
    print("\nüåê Por fuente:")
    for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_images) * 100
        print(f"  ‚Ä¢ {source}: {count} ({percentage:.1f}%)")
    
    # Estimaci√≥n de completitud
    all_cities = load_cities()
    progress = (cities_count / len(all_cities)) * 100
    print(f"\nüéØ Progreso general: {progress:.1f}% ({cities_count}/{len(all_cities)} ciudades)")
    
    print("\n" + "="*70)

# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='Pipeline de miner√≠a de im√°genes para OSINT geolocalizaci√≥n'
    )
    parser.add_argument(
        '--mode',
        choices=['all', 'state', 'city', 'progress'],
        default='all',
        help='Modo de operaci√≥n'
    )
    parser.add_argument('--state', help='Nombre del estado a minar')
    parser.add_argument('--city', help='Nombre de la ciudad a minar')
    parser.add_argument('--images', type=int, default=20, help='Im√°genes por ciudad')
    parser.add_argument('--check-progress', action='store_true', help='Ver progreso')
    
    args = parser.parse_args()
    
    if args.check_progress or args.mode == 'progress':
        check_progress()
        return
    
    if args.mode == 'all':
        mine_all_cities(args.images)
    
    elif args.mode == 'state':
        if not args.state:
            print("‚ùå Debes especificar --state")
            return
        
        cities = get_cities_by_state(args.state)
        if not cities:
            print(f"‚ùå Estado '{args.state}' no encontrado")
            return
        
        print(f"Minando {len(cities)} ciudades de {args.state}...")
        for city in cities:
            mine_city(city['name'], city['state'], city['lat'], city['lon'], args.images)
    
    elif args.mode == 'city':
        if not args.city:
            print("‚ùå Debes especificar --city")
            return
        
        cities = load_cities()
        city_data = next((c for c in cities if c['name'].lower() == args.city.lower()), None)
        
        if not city_data:
            print(f"‚ùå Ciudad '{args.city}' no encontrada")
            return
        
        mine_city(city_data['name'], city_data['state'], city_data['lat'], city_data['lon'], args.images)

if __name__ == '__main__':
    main()
